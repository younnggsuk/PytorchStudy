{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "human-collect",
   "metadata": {},
   "source": [
    "# nn 모듈\n",
    "\n",
    "## PyTorch: nn\n",
    "\n",
    "- Computational graph와 `autograd`는 너무 low-level이라서 큰 신경망에 사용하기는 어려움\n",
    "- PyTorch에서는 computational graph를 higher-level로 추상화하는 패키지로 `nn`을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-third",
   "metadata": {},
   "source": [
    "### Example\n",
    "- nn 패키지를 사용해 2계층 신경망을 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "damaged-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "네트워크 구성\n",
    "\"\"\"\n",
    "\n",
    "# N : batch size\n",
    "# H : hidden layer의 차원\n",
    "# D_in : 입력의 차원\n",
    "# D_out : 출력의 차원\n",
    "N, H, D_in, D_out = 64, 100, 1000, 10\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternate-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "increased-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.448528289794922\n",
      "199 0.044219970703125\n",
      "299 0.0019115060567855835\n",
      "399 0.00011589543282752857\n",
      "499 8.144764251483139e-06\n",
      "\n",
      "0.000228236080147326\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(N, D_in, device=device, dtype=torch.float)\n",
    "y = torch.randn(N, D_out, device=device, dtype=torch.float)\n",
    "\n",
    "# nn.Sequential은 각각의 Module들을 순차적으로 적용해서 출력을 생성\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# mean squared loss (squared L2 norm)\n",
    "# reduction=\"none\" : 벡터 형태\n",
    "# reduction=\"mean\" : 벡터 값들에서 mean 해준 결과, torch.nn.MSELoss(reduction=\"none\").mean()과 동일\n",
    "# reduction=\"mean\" : 벡터 값들 모두 더한 결과, torch.nn.MSELoss(reduction=\"none\").sum()과 동일\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # MSE Loss 계산\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # gradient를 0으로 비워줌\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= lr * param.grad\n",
    "            \n",
    "            \n",
    "print(\"\")\n",
    "print((y_pred - y).sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-decline",
   "metadata": {},
   "source": [
    "## PyTorch: optim\n",
    "\n",
    "- PyTorch의 `optim` 패키지는 최적화 알고리즘에 대한 아이디어를 추상화하고 일반적으로 사용하는 최적화 알고리즘의 구현체(implementation)를 제공\n",
    "    - AdaGrad, RMSProp, Adam 등의 Optimizer 제공\n",
    "\n",
    "### Example\n",
    "- 앞의 예제에 `optim` 패키지가 제공하는 Adam 알고리즘을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "actual-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "네트워크 구성\n",
    "\"\"\"\n",
    "\n",
    "# N : batch size\n",
    "# H : hidden layer의 차원\n",
    "# D_in : 입력의 차원\n",
    "# D_out : 출력의 차원\n",
    "N, H, D_in, D_out = 64, 100, 1000, 10\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "junior-indicator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polished-person",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 51.018287658691406\n",
      "199 0.6328063607215881\n",
      "299 0.0012006131000816822\n",
      "399 1.123887727771944e-06\n",
      "499 6.04595484787751e-10\n",
      "\n",
      "-2.612592652440071e-06\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(N, D_in, device=device, dtype=torch.float)\n",
    "y = torch.randn(N, D_out, device=device, dtype=torch.float)\n",
    "\n",
    "# nn.Sequential은 각각의 Module들을 순차적으로 적용해서 출력을 생성\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# mean squared loss (squared L2 norm)\n",
    "# reduction=\"none\" : 벡터 형태\n",
    "# reduction=\"mean\" : 벡터 값들에서 mean 해준 결과, torch.nn.MSELoss(reduction=\"none\").mean()과 동일\n",
    "# reduction=\"mean\" : 벡터 값들 모두 더한 결과, torch.nn.MSELoss(reduction=\"none\").sum()과 동일\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Adam optimizer 사용\n",
    "# update될 파라미터와 learning rate를 인자로 전달\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # MSE Loss 계산\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # gradient를 0으로 비워줌\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "            \n",
    "\n",
    "print(\"\")\n",
    "print((y_pred - y).sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-lebanon",
   "metadata": {},
   "source": [
    "## PyTorch: 사용자 정의 nn.Module\n",
    "\n",
    "- Sequential하지 않은 복잡한 모델을 구성해야 할 때는 `nn.Module`의 서브클래스로 모듈을 정의할 수 있음\n",
    "    - 서브클래스의 메서드로 `forward()` 를 정의해야 함\n",
    "\n",
    "### Example\n",
    "- 2계층 신경망을 `nn.Module`의 서브클래스로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "raising-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "네트워크 구성\n",
    "\"\"\"\n",
    "\n",
    "# N : batch size\n",
    "# H : hidden layer의 차원\n",
    "# D_in : 입력의 차원\n",
    "# D_out : 출력의 차원\n",
    "N, H, D_in, D_out = 64, 100, 1000, 10\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aerial-address",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "split-above",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNet(\n",
       "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 layer를 정의하고, 멤버 변수로 지정\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "# 모델 정의\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amino-consultancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 59.13008117675781\n",
      "199 0.9483639597892761\n",
      "299 0.0025306076277047396\n",
      "399 2.036195837717969e-06\n",
      "499 4.627082328401144e-10\n",
      "\n",
      "-6.194633897393942e-06\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(N, D_in, device=device, dtype=torch.float)\n",
    "y = torch.randn(N, D_out, device=device, dtype=torch.float)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "            \n",
    "\n",
    "print(\"\")\n",
    "print((y_pred - y).sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-leave",
   "metadata": {},
   "source": [
    "## PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)\n",
    "\n",
    "- Forward pass에서 동일한 Module을 여러번 재사용 가능하므로, 내부 layer들간의 가중치 공유를 구현 가능\n",
    "\n",
    "### Example 1\n",
    "- 동적으로 그래프를 생성하고, 가중치를 재사용하는 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "present-alpha",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-4\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=torch.float)\n",
    "y = torch.randn(N, D_out, device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acute-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        \n",
    "        # 각 forward pass마다 동적으로 graph를 구성\n",
    "        # 동일한 Module(middle_linear)을 여러번 재사용 가능\n",
    "        \n",
    "        count = 0 # 동적으로 생성되는 graph 수 count\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "            count += 1\n",
    "            \n",
    "        print(count)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "human-steam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "model = DynamicNet(D_in, H, D_out)\n",
    "model.to(device)\n",
    "\n",
    "# forward pass에서 동적으로 graph를 생성하는지 확인\n",
    "for t in range(10):\n",
    "    y_pred = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-sculpture",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "- 앞서 구현한 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wrapped-associate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicNet(\n",
       "  (input_linear): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (middle_linear): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (output_linear): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        \n",
    "        # 각 forward pass마다 동적으로 graph를 구성\n",
    "        # 동일한 Module(middle_linear)을 여러번 재사용 가능\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "            \n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "plain-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 20.132814407348633\n",
      "199 1.2074705362319946\n",
      "299 0.23595987260341644\n",
      "399 1.9188899993896484\n",
      "499 0.12006747722625732\n",
      "\n",
      "-0.6344144940376282\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# SGD with momentum=0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "            \n",
    "\n",
    "print(\"\")\n",
    "print((y_pred - y).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-parade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
