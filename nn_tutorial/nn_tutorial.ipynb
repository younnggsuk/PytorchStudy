{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binding-spouse",
   "metadata": {},
   "source": [
    "# TORCH.NN 이 실제로 무엇인가요?\n",
    "\n",
    "- PyTorch는 `torch.nn`, `torch.optim`, `Dataset`, `DataLoader`와 같은 잘 디자인된 모듈과 클래스들을 제공하며, 이들의 기능에 대해 알아본다.\n",
    "- 먼저 PyTorch 텐서(tensor) 기능만을 사용해 MNIST에 대한 기초적인 신경망을 만들어 본 후, `torch.nn`, `torch.optim`, `Dataset`, `DataLoader`를 하나씩 추가하면서, 정확히 각 부분이 어떤 일을 하는지 그리고 이것이 어떻게 코드를 더 정확하고 유연하게 만드는지 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-heaven",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 불러오기\n",
    "\n",
    "- [MNIST](http://deeplearning.net/data/mnist/)는 손으로 쓴 숫자(0에서 9 사이)의 흑백 이미지로 구성된 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "major-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "# 데이터 내려받기\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-bunch",
   "metadata": {},
   "source": [
    "#### `pickle` vs. `json`\n",
    "\n",
    "- 파이썬에서 텍스트뿐만 아니라 객체와 같은 데이터들을 읽고 쓸때 사용할 수 있는 모듈인데, [pickle](https://docs.python.org/ko/3/library/pickle.html)를 보니 보안에 취약한 것 같음\n",
    "- 되도록이면 [json](https://docs.python.org/ko/3/library/json.html#module-json)을 사용하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "private-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# 데이터 불러오기\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "golden-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN8klEQVR4nO3df6jVdZ7H8ddrbfojxzI39iZOrWOEUdE6i9nSyjYRTj8o7FYMIzQ0JDl/JDSwyIb7xxSLIVu6rBSDDtXYMus0UJHFMNVm5S6BdDMrs21qoxjlphtmmv1a9b1/3K9xp+75nOs53/PD+34+4HDO+b7P93zffPHl99f53o8jQgAmvj/rdQMAuoOwA0kQdiAJwg4kQdiBJE7o5sJsc+of6LCI8FjT29qy277C9lu237F9ezvfBaCz3Op1dtuTJP1B0gJJOyW9JGlRROwozMOWHeiwTmzZ50l6JyLejYgvJf1G0sI2vg9AB7UT9hmS/jjq/c5q2p+wvcT2kO2hNpYFoE0dP0EXEeskrZPYjQd6qZ0t+y5JZ4x6/51qGoA+1E7YX5J0tu3v2j5R0o8kbaynLQB1a3k3PiIO2V4q6SlJkyQ9EBFv1NYZgFq1fOmtpYVxzA50XEd+VAPg+EHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEi0P2Yzjw6RJk4r1U045paPLX7p0acPaSSedVJx39uzZxfqtt95arN9zzz0Na4sWLSrO+/nnnxfrK1euLNbvvPPOYr0X2gq77fckHZB0WNKhiJhbR1MA6lfHlv3SiPiwhu8B0EEcswNJtBv2kPS07ZdtLxnrA7aX2B6yPdTmsgC0od3d+PkRscv2X0h6xvZ/R8Tm0R+IiHWS1kmS7WhzeQBa1NaWPSJ2Vc97JD0maV4dTQGoX8thtz3Z9pSjryX9QNL2uhoDUK92duMHJD1m++j3/HtE/L6WriaYM888s1g/8cQTi/WLL764WJ8/f37D2tSpU4vzXn/99cV6L+3cubNYX7NmTbE+ODjYsHbgwIHivK+++mqx/sILLxTr/ajlsEfEu5L+qsZeAHQQl96AJAg7kARhB5Ig7EAShB1IwhHd+1HbRP0F3Zw5c4r1TZs2Feudvs20Xx05cqRYv/nmm4v1Tz75pOVlDw8PF+sfffRRsf7WW2+1vOxOiwiPNZ0tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX2GkybNq1Y37JlS7E+a9asOtupVbPe9+3bV6xfeumlDWtffvllcd6svz9oF9fZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJhmyuwd69e4v1ZcuWFetXX311sf7KK68U683+pHLJtm3bivUFCxYU6wcPHizWzzvvvIa12267rTgv6sWWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4H72PnDyyScX682GF167dm3D2uLFi4vz3njjjcX6hg0binX0n5bvZ7f9gO09trePmjbN9jO2366eT62zWQD1G89u/K8kXfG1abdLejYizpb0bPUeQB9rGvaI2Czp678HXShpffV6vaRr620LQN1a/W38QEQcHSzrA0kDjT5oe4mkJS0uB0BN2r4RJiKidOItItZJWidxgg7opVYvve22PV2Squc99bUEoBNaDftGSTdVr2+S9Hg97QDolKa78bY3SPq+pNNs75T0c0krJf3W9mJJ70v6YSebnOj279/f1vwff/xxy/PecsstxfrDDz9crDcbYx39o2nYI2JRg9JlNfcCoIP4uSyQBGEHkiDsQBKEHUiCsANJcIvrBDB58uSGtSeeeKI47yWXXFKsX3nllcX6008/Xayj+xiyGUiOsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BHfWWWcV61u3bi3W9+3bV6w/99xzxfrQ0FDD2n333Vect5v/NicSrrMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJcZ09ucHCwWH/wwQeL9SlTprS87OXLlxfrDz30ULE+PDxcrGfFdXYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSILr7Cg6//zzi/XVq1cX65dd1vpgv2vXri3WV6xYUazv2rWr5WUfz1q+zm77Adt7bG8fNe0O27tsb6seV9XZLID6jWc3/leSrhhj+r9ExJzq8bt62wJQt6Zhj4jNkvZ2oRcAHdTOCbqltl+rdvNPbfQh20tsD9lu/MfIAHRcq2H/haSzJM2RNCxpVaMPRsS6iJgbEXNbXBaAGrQU9ojYHRGHI+KIpF9KmldvWwDq1lLYbU8f9XZQ0vZGnwXQH5peZ7e9QdL3JZ0mabekn1fv50gKSe9J+mlENL25mOvsE8/UqVOL9WuuuaZhrdm98vaYl4u/smnTpmJ9wYIFxfpE1eg6+wnjmHHRGJPvb7sjAF3Fz2WBJAg7kARhB5Ig7EAShB1Igltc0TNffPFFsX7CCeWLRYcOHSrWL7/88oa1559/vjjv8Yw/JQ0kR9iBJAg7kARhB5Ig7EAShB1IgrADSTS96w25XXDBBcX6DTfcUKxfeOGFDWvNrqM3s2PHjmJ98+bNbX3/RMOWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BDd79uxifenSpcX6ddddV6yffvrpx9zTeB0+fLhYHx4u//XyI0eO1NnOcY8tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX240Cza9mLFo010O6IZtfRZ86c2UpLtRgaGirWV6xYUaxv3LixznYmvKZbdttn2H7O9g7bb9i+rZo+zfYztt+unk/tfLsAWjWe3fhDkv4+Is6V9DeSbrV9rqTbJT0bEWdLerZ6D6BPNQ17RAxHxNbq9QFJb0qaIWmhpPXVx9ZLurZDPQKowTEds9ueKel7krZIGoiIoz9O/kDSQIN5lkha0kaPAGow7rPxtr8t6RFJP4uI/aNrMTI65JiDNkbEuoiYGxFz2+oUQFvGFXbb39JI0H8dEY9Wk3fbnl7Vp0va05kWAdSh6W68bUu6X9KbEbF6VGmjpJskrayeH+9IhxPAwMCYRzhfOffcc4v1e++9t1g/55xzjrmnumzZsqVYv/vuuxvWHn+8/E+GW1TrNZ5j9r+V9GNJr9veVk1brpGQ/9b2YknvS/phRzoEUIumYY+I/5I05uDuki6rtx0AncLPZYEkCDuQBGEHkiDsQBKEHUiCW1zHadq0aQ1ra9euLc47Z86cYn3WrFmttFSLF198sVhftWpVsf7UU08V65999tkx94TOYMsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkuc5+0UUXFevLli0r1ufNm9ewNmPGjJZ6qsunn37asLZmzZrivHfddVexfvDgwZZ6Qv9hyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSaS5zj44ONhWvR07duwo1p988sli/dChQ8V66Z7zffv2FedFHmzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJR0T5A/YZkh6SNCApJK2LiH+1fYekWyT9b/XR5RHxuybfVV4YgLZFxJijLo8n7NMlTY+IrbanSHpZ0rUaGY/9k4i4Z7xNEHag8xqFfTzjsw9LGq5eH7D9pqTe/mkWAMfsmI7Zbc+U9D1JW6pJS22/ZvsB26c2mGeJ7SHbQ+21CqAdTXfjv/qg/W1JL0haERGP2h6Q9KFGjuP/SSO7+jc3+Q5244EOa/mYXZJsf0vSk5KeiojVY9RnSnoyIs5v8j2EHeiwRmFvuhtv25Lul/Tm6KBXJ+6OGpS0vd0mAXTOeM7Gz5f0n5Jel3Skmrxc0iJJczSyG/+epJ9WJ/NK38WWHeiwtnbj60LYgc5reTcewMRA2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLbQzZ/KOn9Ue9Pq6b1o37trV/7kuitVXX29peNCl29n/0bC7eHImJuzxoo6Nfe+rUvid5a1a3e2I0HkiDsQBK9Dvu6Hi+/pF9769e+JHprVVd66+kxO4Du6fWWHUCXEHYgiZ6E3fYVtt+y/Y7t23vRQyO237P9uu1tvR6frhpDb4/t7aOmTbP9jO23q+cxx9jrUW932N5Vrbtttq/qUW9n2H7O9g7bb9i+rZre03VX6Ksr663rx+y2J0n6g6QFknZKeknSoojY0dVGGrD9nqS5EdHzH2DY/jtJn0h66OjQWrb/WdLeiFhZ/Ud5akT8Q5/0doeOcRjvDvXWaJjxn6iH667O4c9b0Yst+zxJ70TEuxHxpaTfSFrYgz76XkRslrT3a5MXSlpfvV6vkX8sXdegt74QEcMRsbV6fUDS0WHGe7ruCn11RS/CPkPSH0e936n+Gu89JD1t+2XbS3rdzBgGRg2z9YGkgV42M4amw3h309eGGe+bddfK8Oft4gTdN82PiL+WdKWkW6vd1b4UI8dg/XTt9BeSztLIGIDDklb1splqmPFHJP0sIvaPrvVy3Y3RV1fWWy/CvkvSGaPef6ea1hciYlf1vEfSYxo57Ognu4+OoFs97+lxP1+JiN0RcTgijkj6pXq47qphxh+R9OuIeLSa3PN1N1Zf3VpvvQj7S5LOtv1d2ydK+pGkjT3o4xtsT65OnMj2ZEk/UP8NRb1R0k3V65skPd7DXv5Evwzj3WiYcfV43fV8+POI6PpD0lUaOSP/P5L+sRc9NOhrlqRXq8cbve5N0gaN7Nb9n0bObSyW9OeSnpX0tqT/kDStj3r7N40M7f2aRoI1vUe9zdfILvprkrZVj6t6ve4KfXVlvfFzWSAJTtABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL/DyJ7caZa7LphAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 확인\n",
    "plt.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dimensional-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Numpy array를 torch.tensor로 변환\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.size()) \n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-boutique",
   "metadata": {},
   "source": [
    "## 2. `torch.nn` 없이 밑바닥부터 신경망 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-component",
   "metadata": {},
   "source": [
    "### 2-1. 신경망 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genetic-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Xaiver initialization : 1/sqrt(n)을 곱해주기\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_() # weights.requires_grad = True와 동일\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "def model(x):\n",
    "    return x.mm(weights) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-brazilian",
   "metadata": {},
   "source": [
    "### 2-2. 출력 및 손실함수 계산\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "$x \\in \\mathbb{R}^n$가 입력으로 주어질 때, softmax 함수는 다음과 같다.\n",
    "\n",
    "$$S(x) = y \\in \\mathbb{R}^n \\ \\ \\text{where}\\ \\ y_i = \\dfrac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$$\n",
    "\n",
    "#### Negative Log-Likelihood Loss (NLL Loss)\n",
    "\n",
    "Negative log-likelihood는 말 그대로 likelihood에 로그를 취한 값의 음의 값을 의미한다. 따라서, likelihood $p$가 입력으로 주어졌을 때 negative log-likelihood 함수는 다음과 같다.\n",
    "\n",
    "$$L(p) = -\\log(p)$$\n",
    "\n",
    "하지만, 우리가 Loss function으로 사용하기 위해서는 정답 index에 해당하는 값만을 optimize해야하므로, target class에 해당하는 값만을 따로 꺼내주는 작업이 필요하다.\n",
    "\n",
    "따라서, $n$개의 class에 대한 likelihood $p \\in \\mathbb{R}^n$와 정답 label의 index에 대한 정보를 담고있는 $c$가 주어졌을 때, NLL Loss는 다음과 같이 계산한다.\n",
    "\n",
    "$$L(p) = -\\log(p_c)$$\n",
    "\n",
    "#### Summary\n",
    "\n",
    "모델의 출력이 주어졌을 때, softmax, NLL을 통해 Loss가 구해지는 전체 과정은 아래 그림과 같다.\n",
    "<img src=\"./assets/nll_loss.jpg\" alt=\"nll_loss\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "governmental-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "softmax\n",
    "\"\"\"\n",
    "def softmax(x):\n",
    "    # (각 element에 지수함수를 취한 값) / (각 element에 지수함수를 취한 값들의 합)\n",
    "    # 이때, 분모는 스칼라이므로 unsqueeze로 차원을 늘린 후 나눠서 broadcasting되도록 함\n",
    "    # \n",
    "    # softmax의 결과는 각 label에 대한 확률값 형태로 나오게 됨\n",
    "    # 즉, 데이터 instance마다 10개의 element로 이루어진 vector인데,\n",
    "    # 이 vector의 각 element들은 모두 해당하는 label에 대한 확률값이 됨\n",
    "    return x.exp() / x.exp().sum(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "uniform-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "negative log-likelihood\n",
    "\"\"\"\n",
    "def nll_loss(probs, targets):\n",
    "    # range(targets.shape()[0]) : 각 data instance의 index\n",
    "    # targets : 정답 label\n",
    "    #\n",
    "    # 즉, 각 instance별로 예측한 값(preds)에 음의 log-softmax를 취한 값들 중에서\n",
    "    # 정답 label 위치의 값들의 평균을 반환하는 것\n",
    "    return -torch.log(probs)[range(targets.shape[0]), targets].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crucial-friday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3802, grad_fn=<NegBackward>)\n",
      "tensor(2.3802, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 64\n",
    "x_batch = x_train[0:batch_size]\n",
    "y_batch = y_train[0:batch_size]\n",
    "\n",
    "out = model(x_batch)\n",
    "y_softmax = softmax(out)\n",
    "loss = nll_loss(y_softmax, y_batch)\n",
    "print(loss)\n",
    "\n",
    "# pytorch의 torch.nn.functional의 nll_loss는 softmax에 log를 취한 값을 input으로 주어야 함\n",
    "print(F.nll_loss(torch.log(y_softmax), y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-technique",
   "metadata": {},
   "source": [
    "### 2-3. 정확도 (accuracy) 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "documentary-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, target):\n",
    "    y_preds = torch.argmax(out, dim=1)\n",
    "    return (y_preds == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "casual-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0938)\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_softmax, y_batch)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-strength",
   "metadata": {},
   "source": [
    "### 2-4. Training Loop\n",
    "\n",
    "매 훈련 루프(training loop)마다 다음의 과정을 반복\n",
    "- 데이터를 mini-batch 단위로 load\n",
    "- 모델을 사용해 prediction\n",
    "- 정답과 비교해 loss 계산\n",
    "- `loss.backward()` 를 이용해 모델 가중치 update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bulgarian-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(out, target):\n",
    "    return nll_loss(softmax(out), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "noble-newark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.3868\n",
      "[Epoch  2] train_loss: 0.3087\n",
      "[Epoch  3] train_loss: 0.2953\n",
      "[Epoch  4] train_loss: 0.2879\n",
      "[Epoch  5] train_loss: 0.2828\n",
      "[Epoch  6] train_loss: 0.2790\n",
      "[Epoch  7] train_loss: 0.2760\n",
      "[Epoch  8] train_loss: 0.2735\n",
      "[Epoch  9] train_loss: 0.2713\n",
      "[Epoch 10] train_loss: 0.2695\n"
     ]
    }
   ],
   "source": [
    "n = x_train.shape[0]\n",
    "n_epochs = 10\n",
    "lr = 0.5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_train_loss = 0\n",
    "    for i in range((n // batch_size) + 1):\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        \n",
    "        x_batch = x_train[start_i:end_i]\n",
    "        y_batch = y_train[start_i:end_i]\n",
    "        \n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weights -= lr * weights.grad\n",
    "            bias -= lr * bias.grad\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            \n",
    "    print(\n",
    "        (f\"[Epoch {epoch + 1:2d}] \" + \n",
    "        f\"train_loss: {(total_train_loss / (n // batch_size + 1)):.4f}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "happy-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9230)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-council",
   "metadata": {},
   "source": [
    "## 3. `torch.nn.functional` 사용하기\n",
    "\n",
    "- PyTorch의 `nn` 클래스를 활용해 더 간결하고 유연하도록 코드를 리팩토링(refactoring)할 수 있음\n",
    "- Activation function, loss function은 `torch.nn.functional`의 함수로 대체 가능 (`torch.nn.functional`은 F namespace로 import 하는것이 관례)\n",
    "    - Softmax, NLL을 사용한 Loss function은 Pytorch의 `F.cross_entropy`으로 대체 (`F.nll_loss`도 있기는 하지만, 2단계의 과정을 1단계로 수행할 수 있어서 `F.cross_entropy`가 더 편리함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mature-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "def model(x):\n",
    "    return x.mm(weights) + bias.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "general-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2732, grad_fn=<NllLossBackward>) tensor(0.9230)\n"
     ]
    }
   ],
   "source": [
    "print(loss_fn(model(x_train), y_train), accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-queens",
   "metadata": {},
   "source": [
    "## 4. `nn.Module` 을 이용하여 리팩토링 하기\n",
    "\n",
    "- `nn.Module` 및 `nn.Parameter`를 사용해 더 명확하고 간결한 train loop를 작성 가능\n",
    "- `nn.Module`의 subclass 형태로 모델을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "political-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mm(self.weights) + self.bias\n",
    "\n",
    "\n",
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pressed-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        total_train_loss = 0\n",
    "        for i in range((n // batch_size) + 1):\n",
    "            start_i = i * batch_size\n",
    "            end_i = start_i + batch_size\n",
    "            \n",
    "            x_batch = x_train[start_i:end_i]\n",
    "            y_batch = y_train[start_i:end_i]\n",
    "            \n",
    "            out = model(x_batch)\n",
    "            loss = loss_fn(out, y_batch)\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            # with torch.no_grad():\n",
    "            #     weights -= lr * weights.grad\n",
    "            #     bias -= lr * bias.grad\n",
    "            #     weights.grad.zero_()\n",
    "            #     bias.grad.zero_()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "                \n",
    "        print(\n",
    "            (f\"[Epoch {epoch + 1:2d}] \" + \n",
    "            f\"train_loss: {(total_train_loss / (n // batch_size + 1)):.4f}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "periodic-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.3860\n",
      "[Epoch  2] train_loss: 0.3087\n",
      "[Epoch  3] train_loss: 0.2953\n",
      "[Epoch  4] train_loss: 0.2879\n",
      "[Epoch  5] train_loss: 0.2828\n",
      "[Epoch  6] train_loss: 0.2790\n",
      "[Epoch  7] train_loss: 0.2760\n",
      "[Epoch  8] train_loss: 0.2735\n",
      "[Epoch  9] train_loss: 0.2713\n",
      "[Epoch 10] train_loss: 0.2695\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "radical-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9229)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-nitrogen",
   "metadata": {},
   "source": [
    "## 5. `optim` 을 이용하여 리팩토링 하기\n",
    "\n",
    "- `torch.optim`은 Pytorch의 다양한 최적화(optimization) 알고리즘을 담고 있는 패키지\n",
    "- optimizer의 `step()`을 사용하면, 각 매개변수를 수동으로 업데이트하는 과정을 생략할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "constitutional-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "expected-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        total_train_loss = 0\n",
    "        for i in range((n // batch_size) + 1):\n",
    "            start_i = i * batch_size\n",
    "            end_i = start_i + batch_size\n",
    "            \n",
    "            x_batch = x_train[start_i:end_i]\n",
    "            y_batch = y_train[start_i:end_i]\n",
    "            \n",
    "            out = model(x_batch)\n",
    "            loss = loss_fn(out, y_batch)\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            #     for p in model.parameters():\n",
    "            #         p -= p.grad * lr\n",
    "            #     model.zero_grad()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        print(\n",
    "            (f\"[Epoch {epoch + 1:2d}] \" + \n",
    "            f\"train_loss: {(total_train_loss / (n // batch_size + 1)):.4f}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "electoral-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.3864\n",
      "[Epoch  2] train_loss: 0.3086\n",
      "[Epoch  3] train_loss: 0.2952\n",
      "[Epoch  4] train_loss: 0.2877\n",
      "[Epoch  5] train_loss: 0.2827\n",
      "[Epoch  6] train_loss: 0.2789\n",
      "[Epoch  7] train_loss: 0.2758\n",
      "[Epoch  8] train_loss: 0.2733\n",
      "[Epoch  9] train_loss: 0.2712\n",
      "[Epoch 10] train_loss: 0.2693\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "german-nightlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9231)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-italian",
   "metadata": {},
   "source": [
    "## 6. `Dataset` 을 이용하여 리팩토링하기\n",
    "\n",
    "- `Dataset` 은 `__len__()` 및 `__getitem__()`를 가진 어떤 것이라도 될 수 있으며, 이 함수들을 indexing하기 위한 방법으로 사용\n",
    "- `TensorDataset` 은 텐서를 감싸는(wrapping) Dataset이며, 길이와 인덱싱 방식을 정의함으로써 텐서의 첫 번째 차원을 따라 반복, 인덱싱 및 슬라이스(slice)하는 방법을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wooden-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "brazilian-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i in range((n // batch_size) + 1):\n",
    "            start_i = i * batch_size\n",
    "            end_i = start_i + batch_size\n",
    "            \n",
    "            # x_batch = x_train[start_i:end_i]\n",
    "            # y_batch = y_train[start_i:end_i]\n",
    "\n",
    "            x_batch, y_batch = train_ds[start_i:end_i]\n",
    "            \n",
    "            out = model(x_batch)\n",
    "            loss = loss_fn(out, y_batch)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        print(\n",
    "            (f\"[Epoch {epoch + 1:2d}] \" + \n",
    "            f\"train_loss: {(total_train_loss / (n // batch_size + 1)):.4f}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "given-bailey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.3863\n",
      "[Epoch  2] train_loss: 0.3086\n",
      "[Epoch  3] train_loss: 0.2953\n",
      "[Epoch  4] train_loss: 0.2879\n",
      "[Epoch  5] train_loss: 0.2828\n",
      "[Epoch  6] train_loss: 0.2790\n",
      "[Epoch  7] train_loss: 0.2760\n",
      "[Epoch  8] train_loss: 0.2735\n",
      "[Epoch  9] train_loss: 0.2713\n",
      "[Epoch 10] train_loss: 0.2695\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "alert-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9229)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-suspect",
   "metadata": {},
   "source": [
    "## 7. `DataLoader`를 이용하여 리팩토링하기\n",
    "\n",
    "- `DataLoader` 는 배치들에 대해서 반복하기 쉽게 만들어주며, 모든 `Dataset` 으로부터 `DataLoader` 를 생성할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "formal-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "clean-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        total_train_loss = 0\n",
    "        # for i in range((n // batch_size) + 1):\n",
    "        #     start_i = i * batch_size\n",
    "        #     end_i = start_i + batch_size\n",
    "        #     x_batch, y_batch = train_ds[start_i:end_i]\n",
    "\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            out = model(x_batch)\n",
    "            loss = loss_fn(out, y_batch)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        print(\n",
    "            (f\"[Epoch {epoch + 1:2d}] \" + \n",
    "            f\"train_loss: {(total_train_loss / len(train_dl)):.4f}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "willing-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.3871\n",
      "[Epoch  2] train_loss: 0.3089\n",
      "[Epoch  3] train_loss: 0.2954\n",
      "[Epoch  4] train_loss: 0.2879\n",
      "[Epoch  5] train_loss: 0.2829\n",
      "[Epoch  6] train_loss: 0.2790\n",
      "[Epoch  7] train_loss: 0.2760\n",
      "[Epoch  8] train_loss: 0.2735\n",
      "[Epoch  9] train_loss: 0.2713\n",
      "[Epoch 10] train_loss: 0.2695\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "educational-revision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9228)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-quantum",
   "metadata": {},
   "source": [
    "## 8. 검증(validation) 추가하기\n",
    "\n",
    "- Validation data의 shuffling은 필요 없음\n",
    "    - Validation loss는 검증 데이터셋을 섞든 안섞든 동일하기 때문\n",
    "    - Train data의 shuffling은 batch와 overfitting 사이의 상관관계를 방지하기 위해 중요\n",
    "- Validation set의 batch size는 train set보다 크게 해서 loss를 빠르게 계산\n",
    "    - Validation set에 대해서는 backprop이 필요하지 않으므로, gradient를 저장하지 않아도 되고 이는 메모리를 덜 사용하게된다는 것을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "chinese-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "endless-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            out = model(x_batch)\n",
    "            loss = loss_fn(out, y_batch)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                out = model(x_batch)\n",
    "                total_valid_loss += loss_fn(out, y_batch).item()\n",
    "                \n",
    "        print(\n",
    "            (f\"[Epoch {epoch + 1:2d}] \" + \n",
    "            f\"train_loss: {(total_train_loss / len(train_dl)):.4f} / \" +\n",
    "            f\"valid_loss: {(total_valid_loss / len(valid_dl)):.4f}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "large-tract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.3899 / valid_loss: 0.2909\n",
      "[Epoch  2] train_loss: 0.3114 / valid_loss: 0.3031\n",
      "[Epoch  3] train_loss: 0.2974 / valid_loss: 0.2908\n",
      "[Epoch  4] train_loss: 0.2910 / valid_loss: 0.2835\n",
      "[Epoch  5] train_loss: 0.2852 / valid_loss: 0.2782\n",
      "[Epoch  6] train_loss: 0.2811 / valid_loss: 0.2784\n",
      "[Epoch  7] train_loss: 0.2792 / valid_loss: 0.2782\n",
      "[Epoch  8] train_loss: 0.2767 / valid_loss: 0.2697\n",
      "[Epoch  9] train_loss: 0.2733 / valid_loss: 0.2779\n",
      "[Epoch 10] train_loss: 0.2727 / valid_loss: 0.3184\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "color-employment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9071)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-brave",
   "metadata": {},
   "source": [
    "## 9. `fit()` 와 `get_data()` 생성하기\n",
    "\n",
    "- `loss_batch()`\n",
    "    - Train과 validation에 대해 loss를 계산하는 process가 동일하므로, refactoring한 것\n",
    "    - 하나의 mini-batch에 대해 loss를 계산\n",
    "- `fit()`\n",
    "    - 모델을 훈련하고 각 epoch에 대한 train과 validation loss를 계산\n",
    "- `get_data()`\n",
    "    - train과 validation set에 대한 `DataLoader`를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "voluntary-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_fn, x_batch, y_batch, optimizer=None):\n",
    "    loss = loss_fn(model(x_batch), y_batch)\n",
    "\n",
    "    if optimizer:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "disturbed-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(n_epochs, model, loss_fn, optimizer, train_dl, valid_dl):\n",
    "    for epoch in range(n_epochs):\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            loss = loss_batch(model, loss_fn, x_batch, y_batch, optimizer)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                loss = loss_batch(model, loss_fn, x_batch, y_batch)\n",
    "                valid_losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            (f\"[Epoch {epoch + 1:2d}] \" + \n",
    "            f\"train_loss: {np.mean(train_losses):.4f} / \" +\n",
    "            f\"valid_loss: {np.mean(valid_losses):.4f}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "musical-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, batch_size):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "outdoor-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.3931 / valid_loss: 0.2941\n",
      "[Epoch  2] train_loss: 0.3124 / valid_loss: 0.3320\n",
      "[Epoch  3] train_loss: 0.2981 / valid_loss: 0.2767\n",
      "[Epoch  4] train_loss: 0.2911 / valid_loss: 0.2910\n",
      "[Epoch  5] train_loss: 0.2858 / valid_loss: 0.2837\n",
      "[Epoch  6] train_loss: 0.2810 / valid_loss: 0.2628\n",
      "[Epoch  7] train_loss: 0.2785 / valid_loss: 0.2733\n",
      "[Epoch  8] train_loss: 0.2762 / valid_loss: 0.2679\n",
      "[Epoch  9] train_loss: 0.2740 / valid_loss: 0.3522\n",
      "[Epoch 10] train_loss: 0.2731 / valid_loss: 0.2797\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch_size)\n",
    "model, optimizer = get_model()\n",
    "fit(n_epochs, model, loss_fn, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "religious-cigarette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9234)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-sound",
   "metadata": {},
   "source": [
    "## 10. CNN 으로 넘어가기\n",
    "\n",
    "- 3개의 convolutional layer로 모델 구현\n",
    "    - `nn.Conv2d`\n",
    "    - `F.relu`\n",
    "    - `F.avg_pool2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "closing-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = x.view(-1, x.size(1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "mysterious-queens",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 1.2535 / valid_loss: 0.6766\n",
      "[Epoch  2] train_loss: 0.6440 / valid_loss: 0.5624\n",
      "[Epoch  3] train_loss: 0.5607 / valid_loss: 0.5280\n",
      "[Epoch  4] train_loss: 0.2797 / valid_loss: 0.2574\n",
      "[Epoch  5] train_loss: 0.2443 / valid_loss: 0.2228\n",
      "[Epoch  6] train_loss: 0.2174 / valid_loss: 0.2155\n",
      "[Epoch  7] train_loss: 0.2048 / valid_loss: 0.1919\n",
      "[Epoch  8] train_loss: 0.1920 / valid_loss: 0.1831\n",
      "[Epoch  9] train_loss: 0.1857 / valid_loss: 0.1705\n",
      "[Epoch 10] train_loss: 0.1739 / valid_loss: 0.1625\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "\n",
    "model = Mnist_CNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(n_epochs, model, loss_fn, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sapphire-notion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9509)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-purpose",
   "metadata": {},
   "source": [
    "## 11. `nn.Sequential`\n",
    "\n",
    "- `Sequential`은 그 안에 포함된 각 모듈을 순차적으로 실행하며, 이를 사용하면 신경망을 쉽게 구현할 수 있음\n",
    "- 여기에 원하는 기능을 넣기 위해서는, 사용자정의 레이어(custom layer)를 정의해야 함\n",
    "    - 예를 들어, PyTorch에는 view 레이어가 없으므로 직접 custom layer를 만들어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "different-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "professional-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.9041 / valid_loss: 0.3840\n",
      "[Epoch  2] train_loss: 0.3575 / valid_loss: 0.2872\n",
      "[Epoch  3] train_loss: 0.2710 / valid_loss: 0.2407\n",
      "[Epoch  4] train_loss: 0.2233 / valid_loss: 0.2086\n",
      "[Epoch  5] train_loss: 0.2045 / valid_loss: 0.1939\n",
      "[Epoch  6] train_loss: 0.1860 / valid_loss: 0.1863\n",
      "[Epoch  7] train_loss: 0.1707 / valid_loss: 0.1609\n",
      "[Epoch  8] train_loss: 0.1620 / valid_loss: 0.1522\n",
      "[Epoch  9] train_loss: 0.1514 / valid_loss: 0.1623\n",
      "[Epoch 10] train_loss: 0.1479 / valid_loss: 0.1290\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(-1, x.size(1)))\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(n_epochs, model, loss_fn, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "minor-tolerance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9616)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-halifax",
   "metadata": {},
   "source": [
    "## 12. DataLoader 감싸기\n",
    "\n",
    "- `preprocess()`를 통해 전처리해주던 과정을 제너레이터로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "remarkable-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrapperDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield self.func(*b)\n",
    "            \n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch_size=batch_size)\n",
    "train_dl = WrapperDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrapperDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "senior-calgary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.9299 / valid_loss: 0.3762\n",
      "[Epoch  2] train_loss: 0.3735 / valid_loss: 0.2807\n",
      "[Epoch  3] train_loss: 0.2791 / valid_loss: 0.2216\n",
      "[Epoch  4] train_loss: 0.2362 / valid_loss: 0.1809\n",
      "[Epoch  5] train_loss: 0.2046 / valid_loss: 0.1670\n",
      "[Epoch  6] train_loss: 0.1875 / valid_loss: 0.1644\n",
      "[Epoch  7] train_loss: 0.1708 / valid_loss: 0.1381\n",
      "[Epoch  8] train_loss: 0.1560 / valid_loss: 0.1562\n",
      "[Epoch  9] train_loss: 0.1469 / valid_loss: 0.1574\n",
      "[Epoch 10] train_loss: 0.1404 / valid_loss: 0.1475\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(-1, x.size(1)))\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(n_epochs, model, loss_fn, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-blues",
   "metadata": {},
   "source": [
    "## 12. GPU 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "typical-gathering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "banner-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(device), y.to(device)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch_size=batch_size)\n",
    "train_dl = WrapperDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrapperDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sorted-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] train_loss: 0.8737 / valid_loss: 0.3166\n",
      "[Epoch  2] train_loss: 0.3208 / valid_loss: 0.2550\n",
      "[Epoch  3] train_loss: 0.2632 / valid_loss: 0.2293\n",
      "[Epoch  4] train_loss: 0.2324 / valid_loss: 0.1887\n",
      "[Epoch  5] train_loss: 0.2126 / valid_loss: 0.1963\n",
      "[Epoch  6] train_loss: 0.1965 / valid_loss: 0.1781\n",
      "[Epoch  7] train_loss: 0.1802 / valid_loss: 0.1871\n",
      "[Epoch  8] train_loss: 0.1734 / valid_loss: 0.1668\n",
      "[Epoch  9] train_loss: 0.1635 / valid_loss: 0.1482\n",
      "[Epoch 10] train_loss: 0.1564 / valid_loss: 0.1443\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(-1, x.size(1)))\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(n_epochs, model, loss_fn, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-branch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
