{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "middle-functionality",
   "metadata": {},
   "source": [
    "# AUTOGRAD: 자동 미분\n",
    "\n",
    "### `autograd` 패키지\n",
    "- Tensor의 모든 연산에 대해 자동 미분을 제공\n",
    "- 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의\n",
    "\n",
    "\n",
    "### `torch.Tensor` 클래스\n",
    "- `.requires_grad=True`로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)\n",
    "- 모든 계산이 완료된 후 `.backward()` 를 호출하여 모든 gradient를 자동으로 계산\n",
    "- Tensor의 gradient는 `.grad` 속성에 누적됨\n",
    "- Tensor가 연산을 추적하는 것을 중단하게 하려면, `.detach()` 를 호출\n",
    "- 코드 블럭을 `with torch.no_grad()`으로 감싸면, 기록을 추적하는 것(과 메모리를 사용하는 것)을 방지할 수 있음\n",
    "    - 특히 `gradient`를 추적할 필요가 없는 상황인 모델을 평가(evaluate)할 때 유용\n",
    "\n",
    "\n",
    "### `Function` 클래스\n",
    "- gradient를 계산하기 위한 연산을 정의\n",
    "    - 각 tensor는 `.grad_fn` 속성을 갖고 있는데, 이는 Tensor를 생성한 Function 을 참조 (단, 사용자가 만든 Tensor는 예외 : 이때 `grad_fn` 은 `None`)\n",
    "- 도함수를 계산하기 위해서는 `Tensor` 의 `.backward()`를 호출하면 됨\n",
    "    - `Tensor` 가 스칼라(scalar)인 경우(예. 하나의 요소 값만 갖는 등)에는 `backward`에 인자를 정해주지 않아도 됨\n",
    "    - `Tensor` 가 여러 개의 요소를 갖고 있을 때는 tensor의 모양을 `gradient` 인자로 전달해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-grass",
   "metadata": {},
   "source": [
    "# Gradient 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "according-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "separated-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7f6518efbfa0>\n"
     ]
    }
   ],
   "source": [
    "# 기본 tensor 생성 시, requires_grad는 False\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "\n",
    "# .requires_grad_(True)로 requires_grad값을 inplace로 변경 가능\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "# 이제 연산을 추적하므로, grad_fn이 생겼음(Summation)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-gothic",
   "metadata": {},
   "source": [
    "# Gradient 계산\n",
    "\n",
    "#### `Tensor`가 스칼라(scalar)인 경우\n",
    "- `backward()`에 인자 전달 안해도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educated-advocate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# requires_grad = True로 연산 추적\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "common-convenience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7f65186b75b0>\n"
     ]
    }
   ],
   "source": [
    "# grad_fn이 Addition\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attended-browse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x7f65186b75b0>\n"
     ]
    }
   ],
   "source": [
    "# grad_fn이 Multiplication\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "psychological-coordination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "<MeanBackward0 object at 0x7f648abc3e50>\n"
     ]
    }
   ],
   "source": [
    "# grad_fn이 Mean\n",
    "out = z.mean()\n",
    "print(out)\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acting-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# out은 scalar이므로, backward()에 인자 전달 안해도 됨\n",
    "\n",
    "# out.backward()는 out.backward(torch.tensor(1., dtype=torch.float))와 동일\n",
    "# 또한, 이는 구해지는 gradient에 1을 곱한것과 결과가 같음\n",
    "# out.backward()\n",
    "out.backward(torch.tensor(1., dtype=torch.float))\n",
    "\n",
    "# `.grad`로 output에 대한 해당 변수의 gradient를 구할 수 있음\n",
    "print(x.grad) # d(out)/dx : Jacobian Matrix(vector간 gradient라서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "russian-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "# gradient에 1을 곱한것과 결과가 같음 (위 자코비안에 2를 곱한 결과)\n",
    "out.backward(torch.tensor(2., dtype=torch.float))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-neighbor",
   "metadata": {},
   "source": [
    "#### `Tensor`가 여러 개의 요소(vector)인 경우\n",
    "\n",
    "- `backward()`의 `gradient` 인자로 vector를 전달해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reduced-peninsula",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 484.8318,  173.9296, 1075.4950], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "out = x * 2\n",
    "while out.data.norm() < 1000:\n",
    "    out = out * 2\n",
    "\n",
    "# out은 vector 형태(앞의 예제와 달리, scalar가 아님)\n",
    "# 크기는 3x1\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inside-burton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "# 3x1 크기의 vector를 gradient 인자로 전달해줘야 함\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "out.backward(gradient=v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-potter",
   "metadata": {},
   "source": [
    "# 추가) CS231n 4강의 gradient 예제\n",
    "\n",
    "<img src=\"./assets/cs231n_lecture_4_backprop_vectorized_form.png\" alt=\"cs231n_lecture_4_backprop_vectorized_form\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "timely-drink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1000,  0.5000],\n",
      "        [-0.3000,  0.8000]], requires_grad=True)\n",
      "tensor([[0.2000],\n",
      "        [0.4000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.tensor([[0.1, 0.5], [-0.3, 0.8]], dtype=torch.float, requires_grad=True)\n",
    "x = torch.tensor([[0.2], [0.4]], dtype=torch.float, requires_grad=True)\n",
    "print(W)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "democratic-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2200],\n",
      "        [0.2600]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(W, x)\n",
    "z.retain_grad() # z는 말단 노드가 아니라서, .retain_grad()를 호출해주어야 gradient를 보존할 수 있음\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "manufactured-pattern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1160, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = z.norm() ** 2\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "burning-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "undefined-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0880, 0.1760],\n",
      "        [0.1040, 0.2080]])\n",
      "tensor([[-0.1120],\n",
      "        [ 0.6360]])\n",
      "tensor([[0.4400],\n",
      "        [0.5200]])\n"
     ]
    }
   ],
   "source": [
    "# CS231n ppt에서와 동일한 결과를 확인할 수 있음\n",
    "print(W.grad)\n",
    "print(x.grad)\n",
    "print(z.grad) # z.retain_grad() 안해주면 출력이 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-teaching",
   "metadata": {},
   "source": [
    "# Gradient 추적 없이 코드블럭 감싸기\n",
    "\n",
    "- `with torch.no_grad()`\n",
    "- 모델을 평가(evaluate)할 때 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "champion-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "# 생성되는 tensor에 대해서만 추적을 하지 않음\n",
    "# 따라서, evaluation 과정에서 계산한 output에 대한 gradient 추적만을 제한 가능\n",
    "# (학습한 모델의 전체 파라미터에 대한 gradient 추적은 멈추지 않고,\n",
    "#  prediction한 결과에 대해서만 추적 하지 않을 수 있음)\n",
    "with torch.no_grad():\n",
    "    print(x.requires_grad) # True\n",
    "    print((x ** 2).requires_grad) # False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "interracial-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "\n",
    "# .detach() 호출 시, requires_grad가 False가 됨\n",
    "y = x.detach()\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "educational-direction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True]])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# .detach()한 데이터와 tensor의 데이터는 같음\n",
    "print(x.eq(y))\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-cheat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
